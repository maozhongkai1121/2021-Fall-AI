{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from data import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os. environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **set params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **load train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_vert = np.load('../../data2/news_vert.npy')\n",
    "news_title = np.load('../../data2/news_title.npy')\n",
    "news_body = np.load('../../data2/news_body.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data2/TrainUsers.pkl', 'rb') as f:\n",
    "    TrainUsers = pickle.load(f)\n",
    "with open('../../data2/TrainSamples.pkl', 'rb') as f:\n",
    "    TrainSamples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data2/dict.pkl', 'rb') as f:\n",
    "    _,category_dict,word_dict = pickle.load(f)\n",
    "with open('../../data2/news.pkl', 'rb') as f:\n",
    "    news = pickle.load(f)\n",
    "embedding_matrix = np.load('../../data2/embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NRMS(embedding_matrix)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **begin training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_hat):\n",
    "    y_hat = torch.argmax(y_hat, dim=-1)\n",
    "    tot = y_true.shape[0]\n",
    "    hit = torch.sum(y_true == y_hat)\n",
    "    return hit.data.float() * 1.0 / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001) #lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5963 [00:00<?, ?it/s]d:\\Code\\Fundamental_AI\\PENS-Personalized-News-Headline-Generation\\pensmodule\\UserEncoder\\data.py:66: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  user_feature = [torch.LongTensor(i) for i in zip(*user_feature)]\n",
      "ed: 762880, train_loss: 0.59068, acc: 0.67883: 100%|██████████| 5963/5963 [30:30<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(0.5906, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ed: 762880, train_loss: 0.55464, acc: 0.71287: 100%|██████████| 5963/5963 [30:09<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor(0.5545, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ed: 762880, train_loss: 0.54268, acc: 0.72221: 100%|██████████| 5963/5963 [30:23<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tensor(0.5426, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "min_train_loss = 100.0\n",
    "for ep in range(1,4):\n",
    "    loss = 0.0\n",
    "    accuary = 0.0\n",
    "    cnt = 1\n",
    "    dset = TrainDataset(TrainUsers, TrainSamples, news_title, news_vert, news_body)\n",
    "    data_loader = DataLoader(dset, batch_size=128, collate_fn=collate_fn, shuffle=True)\n",
    "    tqdm_util = tqdm(data_loader)\n",
    "    for user_feature, news_feature, label in tqdm_util: \n",
    "        user_feature = [i.to(device) for i in user_feature]\n",
    "        news_feature = [i.to(device) for i in news_feature]\n",
    "        label = label.to(device)\n",
    "        bz_loss, y_hat = model(user_feature, news_feature, label)\n",
    "        loss += bz_loss.data.float()\n",
    "        accuary += acc(label, y_hat)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        bz_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if cnt % 10 == 0:\n",
    "            tqdm_util.set_description('ed: {}, train_loss: {:.5f}, acc: {:.5f}'.format(cnt * batch_size, loss.data / cnt, accuary / cnt))\n",
    "        cnt += 1\n",
    "    loss /= cnt\n",
    "    print(ep, loss)\n",
    "    torch.save(model.state_dict(), '../../runs/userencoder/NAML-{}.pkl'.format(ep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **load test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data2/ValidUsers.pkl', 'rb') as f:\n",
    "    ValidUsers = pickle.load(f)\n",
    "with open('../../data2/ValidSamples.pkl', 'rb') as f:\n",
    "    ValidSamples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:02<00:00, 79.40it/s]\n",
      "100%|██████████| 782/782 [00:02<00:00, 282.80it/s]\n",
      "100%|██████████| 100000/100000 [01:41<00:00, 982.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "1\n",
      "AUC\t MRR\t nDCG5\t nDCG10\t CTR1\t CTR10\t\n",
      "(0.6436104504160457, 0.23353615763002544, 0.25621876444997116, 0.3346347817845591, 0.14899, 0.12019733333333334)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:02<00:00, 77.53it/s]\n",
      "100%|██████████| 782/782 [00:02<00:00, 333.08it/s]\n",
      "100%|██████████| 100000/100000 [01:37<00:00, 1023.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "2\n",
      "AUC\t MRR\t nDCG5\t nDCG10\t CTR1\t CTR10\t\n",
      "(0.645814365689914, 0.23421902581878667, 0.2573015718911829, 0.3355176226438488, 0.14898, 0.12062533333333333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:02<00:00, 76.30it/s]\n",
      "100%|██████████| 782/782 [00:02<00:00, 350.02it/s]\n",
      "100%|██████████| 100000/100000 [01:44<00:00, 958.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "3\n",
      "AUC\t MRR\t nDCG5\t nDCG10\t CTR1\t CTR10\t\n",
      "(0.6491278531495707, 0.23887359880774814, 0.2636869254022473, 0.3417275924090409, 0.15598, 0.12316933333333333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(1,4):\n",
    "    model = NRMS(embedding_matrix)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load('../../runs/userencoder/NAML-{}.pkl'.format(ep)))\n",
    "    model.eval() \n",
    "    # save new embedding matrix\n",
    "    np.save('../../data2/embedding_matrix{}.npy'.format(ep), model.embed.weight.data.cpu().numpy())\n",
    "    \n",
    "\n",
    "    n_dset = news_dataset(news_title, news_vert, news_body)\n",
    "    news_data_loader = DataLoader(n_dset, batch_size=512, collate_fn=news_collate_fn, shuffle=False)\n",
    "\n",
    "      \n",
    "    news_scoring = []\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        for news_feature in tqdm(news_data_loader): \n",
    "            news_feature = [i.to(device) for i in news_feature]\n",
    "            news_vec = model.news_encoder(news_feature)\n",
    "            news_vec = news_vec.to(torch.device(\"cpu\")).detach().numpy()\n",
    "            news_scoring.extend(news_vec)\n",
    "    news_scoring = np.array(news_scoring)\n",
    "    np.save('../../data2/news_scoring{}.npy'.format(ep), news_scoring)\n",
    "    \n",
    "    u_dset = UserDataset(news_scoring, ValidUsers)\n",
    "    user_data_loader = DataLoader(u_dset, batch_size=128, shuffle=False)\n",
    "\n",
    "    user_scoring = []\n",
    "    with torch.no_grad():\n",
    "        for user_feature in tqdm(user_data_loader): \n",
    "            user_feature = user_feature.to(device)\n",
    "            user_vec = model.user_encoder(user_feature)\n",
    "            user_vec = user_vec.to(torch.device(\"cpu\")).detach().numpy()\n",
    "            user_scoring.extend(user_vec)\n",
    "    user_scoring = np.array(user_scoring)\n",
    "    np.save('../../data2/global_user_embed{}.npy'.format(ep),np.mean(user_scoring,axis=0))\n",
    "    g = evaluate(user_scoring,news_scoring, ValidSamples)\n",
    "    print(ep)\n",
    "    print('AUC\\t', 'MRR\\t', 'nDCG5\\t', 'nDCG10\\t','CTR1\\t','CTR10\\t')\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f39a3724a8be2f5aa2e45c3d3b1b9485fd89a76ca170e01e887c3ee7bf39cb5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('py3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
