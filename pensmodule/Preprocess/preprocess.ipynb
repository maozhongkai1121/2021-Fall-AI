{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfilename = '../../data/train.tsv'\n",
    "validfilename = '../../data/valid.tsv'\n",
    "testfilename = '../../data/personalized_test.tsv'\n",
    "docsfilename = '../../data/news.tsv'\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_FREQ_THRESHOLD = 3\n",
    "MAX_CONTENT_LEN = 500\n",
    "MAX_BODY_LEN = 100\n",
    "MAX_TITLE_LEN = 16\n",
    "WORD_EMBEDDING_DIM = 300\n",
    "MAX_CLICK_LEN = 50\n",
    "\n",
    "word2freq = {}\n",
    "word2index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sent):\n",
    "    pat = re.compile(r'[\\w]+|[.,!?;|]')\n",
    "    if isinstance(sent, str):\n",
    "        return pat.findall(sent.lower())\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_news(filename,filer_num=3):\n",
    "    news={}\n",
    "    category, subcategory=[], []\n",
    "    news_index={}\n",
    "    index=1\n",
    "    word_cnt=Counter()\n",
    "    err = 0\n",
    "    news_data = pd.read_csv(filename, sep='\\t')\n",
    "    news_data.fillna(value=\" \", inplace=True)\n",
    "    for i in tqdm(range(len(news_data))):\n",
    "        doc_id,vert,_, title, snipplet= news_data.loc[i,:][:5]\n",
    "        news_index[doc_id]=index\n",
    "        index+=1\n",
    "\n",
    "        title = title.lower()\n",
    "        title = word_tokenize(title)\n",
    "        snipplet = snipplet.lower()\n",
    "        snipplet = word_tokenize(snipplet)\n",
    "        category.append(vert)\n",
    "        news[doc_id] = [vert,title,snipplet]     \n",
    "        word_cnt.update(snipplet+title)\n",
    "    # 0: pad; 1: <sos>; 2: <eos>\n",
    "    word = [k for k , v in word_cnt.items() if v >= filer_num]\n",
    "    word_dict = {k:v for k, v in zip(word, range(3,len(word)+3))}\n",
    "    category=list(set(category))\n",
    "    category_dict={k:v for k, v in zip(category, range(1,len(category)+1))}\n",
    "\n",
    "    return news,news_index,category_dict,word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113762/113762 [01:19<00:00, 1434.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%time news,news_index,category_dict,word_dict = read_news(docsfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict['unk'] = 0\n",
    "word_dict['<sos>'] = 1\n",
    "word_dict['<eos>'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data2/dict.pkl', 'wb') as f:\n",
    "    pickle.dump([news_index,category_dict,word_dict], f)\n",
    "with open('../../data2/news.pkl', 'wb') as f:\n",
    "    pickle.dump(news, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get inputs for user encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_for_userencoder(news,news_index,category_dict,word_dict):\n",
    "    news_num=len(news)+1\n",
    "    news_title=np.zeros((news_num,MAX_TITLE_LEN),dtype='int32')\n",
    "    news_body=np.zeros((news_num,MAX_BODY_LEN),dtype='int32')\n",
    "    news_vert=np.zeros((news_num),dtype='int32')\n",
    "    for key in news:    \n",
    "        vert,title,body=news[key]\n",
    "        doc_index=news_index[key]\n",
    "        news_vert[doc_index] = category_dict[vert]\n",
    "        counter = 0\n",
    "        for word_id in range(min(MAX_TITLE_LEN,len(title))):\n",
    "            if title[word_id] in word_dict:\n",
    "                news_title[doc_index,counter]=word_dict[title[word_id].lower()]\n",
    "                counter += 1\n",
    "        counter = 0\n",
    "        for word_id in range(min(MAX_BODY_LEN,len(body))):\n",
    "            if body[word_id] in word_dict:\n",
    "                news_body[doc_index,counter]=word_dict[body[word_id].lower()]\n",
    "                counter += 1\n",
    "    return news_vert, news_title, news_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.09 s\n"
     ]
    }
   ],
   "source": [
    "%time news_vert, news_title, news_body = get_rep_for_userencoder(news,news_index,category_dict,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113763, 113763, 113763)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_vert),len(news_title), len(news_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data2/news_vert.npy', news_vert)\n",
    "np.save('../../data2/news_title.npy', news_title)\n",
    "np.save('../../data2/news_body.npy', news_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get inputs/ targets for seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_for_seq2seq(news,news_index,word_dict):\n",
    "    news_num=len(news)+1\n",
    "    sources=np.zeros((news_num,MAX_CONTENT_LEN),dtype='int32')\n",
    "    target_inputs=np.zeros((news_num,MAX_TITLE_LEN),dtype='int32')\n",
    "    target_outputs=np.zeros((news_num,MAX_TITLE_LEN),dtype='int32')\n",
    "    for key in tqdm(news):    \n",
    "        _, title, body = news[key]\n",
    "        doc_index=news_index[key]\n",
    "        counter = 0\n",
    "        for word_id in range(min(MAX_CONTENT_LEN-1,len(body))):\n",
    "            if body[word_id] in word_dict:\n",
    "                sources[doc_index,counter]=word_dict[body[word_id].lower()]\n",
    "                counter += 1\n",
    "        sources[doc_index,counter] = 2 \n",
    "        \n",
    "        target_inputs[doc_index,0] = 1\n",
    "        counter = 1\n",
    "        for word_id in range(min(MAX_TITLE_LEN-1,len(title))):\n",
    "            if title[word_id] in word_dict:\n",
    "                target_inputs[doc_index,counter]=word_dict[title[word_id].lower()]\n",
    "                counter += 1\n",
    "        \n",
    "        counter = 0\n",
    "        for word_id in range(min(MAX_TITLE_LEN-1,len(title))):\n",
    "            if title[word_id] in word_dict:\n",
    "                target_outputs[doc_index,counter]=word_dict[title[word_id].lower()]\n",
    "                counter += 1\n",
    "        target_outputs[doc_index,counter] = 2\n",
    "        \n",
    "    return sources, target_inputs, target_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113762/113762 [00:18<00:00, 6098.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time sources, target_inputs, target_outputs = get_rep_for_seq2seq(news,news_index,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data2/sources.npy', sources)\n",
    "np.save('../../data2/target_inputs.npy', target_inputs)\n",
    "np.save('../../data2/target_outputs.npy', target_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrix(embedding_path,word_dict):\n",
    "    mu, sigma = 0, 0.1\n",
    "    embedding_zero = np.zeros((1,300))\n",
    "    embedding_matrix = np.random.normal(mu, sigma, (len(word_dict)-1, WORD_EMBEDDING_DIM))\n",
    "    embedding_matrix = np.concatenate((embedding_zero,embedding_matrix))\n",
    "    have_word=[]\n",
    "    with open(os.path.join(embedding_path,'glove.840B.300d.txt'),'rb') as f:\n",
    "        while True:\n",
    "            l=f.readline()\n",
    "            if len(l)==0:\n",
    "                break\n",
    "            l=l.split()\n",
    "            word = l[0].decode()\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "                tp = [float(x) for x in l[1:]]\n",
    "                embedding_matrix[index]=np.array(tp)\n",
    "                have_word.append(word)\n",
    "    return embedding_matrix,have_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.4 s\n"
     ]
    }
   ],
   "source": [
    "%time embedding_matrix, have_word = load_matrix('../../data',word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141910, 100875)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict),len(have_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data2/embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get train/ valid/ test examples from user logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doc2ID(doclist,news2id):\n",
    "    return [news2id[i] for i in doclist if i in news2id ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PadDoc(doclist):\n",
    "    if len(doclist) >= MAX_CLICK_LEN:\n",
    "        return doclist[-MAX_CLICK_LEN:]\n",
    "    else:\n",
    "        return [0] * (MAX_CLICK_LEN-len(doclist)) + doclist[:MAX_CLICK_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user2dict(users):\n",
    "    user_set = set(users)\n",
    "    user_dict = {k:v for k, v in zip(user_set, range(0,len(user_set)))}\n",
    "    return user_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_user(filename,news_index):\n",
    "        \n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    df.fillna(value=\" \", inplace=True)\n",
    "    \n",
    "    df['ClicknewsID'] = df['ClicknewsID'].apply(lambda x: PadDoc(Doc2ID(x.split(),news_index)))\n",
    "    \n",
    "    df['pos']  = df['pos'].apply(lambda x: Doc2ID(x.split(),news_index))\n",
    "    df['neg'] = df['neg'].apply(lambda x: Doc2ID(x.split(),news_index))\n",
    "    \n",
    "    pos_neg_lists = []\n",
    "    for userindex, (pos_list, neg_list) in tqdm(enumerate(zip(df['pos'].values.tolist(), df['neg'].values.tolist()))):\n",
    "        if len(pos_list) and len(neg_list):\n",
    "            # sampling 1 negative sample for 1 pos sample\n",
    "            min_len = min(len(pos_list), len(neg_list))\n",
    "            np.random.shuffle(pos_list)\n",
    "            np.random.shuffle(neg_list)\n",
    "            for i in range(min_len):\n",
    "                pos_neg_lists.append([userindex, [pos_list[i],neg_list[i]],[1,0]])\n",
    "        \n",
    "    return df['ClicknewsID'].values.tolist(), pos_neg_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:06, 63572.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%time TrainUsers, TrainSamples = parse_train_user(trainfilename, news_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data2/TrainUsers.pkl', 'wb') as f:\n",
    "    pickle.dump(TrainUsers, f)\n",
    "with open('../../data2/TrainSamples.pkl', 'wb') as f:\n",
    "    pickle.dump(TrainSamples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_valid_user(filename,news_index):\n",
    "        \n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    df.fillna(value=\" \", inplace=True)\n",
    "    \n",
    "    df['ClicknewsID'] = df['ClicknewsID'].apply(lambda x: PadDoc(Doc2ID(x.split(),news_index)))\n",
    "    \n",
    "    df['pos']  = df['pos'].apply(lambda x: Doc2ID(x.split(),news_index))\n",
    "    df['neg'] = df['neg'].apply(lambda x: Doc2ID(x.split(),news_index))\n",
    "    \n",
    "    pos_neg_lists = []\n",
    "    for userindex, (pos_list, neg_list) in enumerate(zip(df['pos'].values.tolist(), df['neg'].values.tolist())):\n",
    "        if len(pos_list) and len(neg_list):\n",
    "            pos_neg_lists.append([userindex, pos_list+neg_list,[1]*len(pos_list)+[0]*len(neg_list)])\n",
    "        \n",
    "    return df['ClicknewsID'].values.tolist(), pos_neg_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%time ValidUsers, ValidSamples = parse_valid_user(validfilename,news_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data2/ValidUsers.pkl', 'wb') as f:\n",
    "    pickle.dump(ValidUsers, f)\n",
    "with open('../../data2/ValidSamples.pkl', 'wb') as f:\n",
    "    pickle.dump(ValidSamples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_user(filename,news_index):\n",
    "        \n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    \n",
    "    df['clicknewsID'] = df['clicknewsID'].apply(lambda x: PadDoc(Doc2ID(x.split(','),news_index)))\n",
    "    \n",
    "    df['posnewID']  = df['posnewID'].apply(lambda x: Doc2ID(x.split(','),news_index))\n",
    "    \n",
    "    df['rewrite_titles'] = df['rewrite_titles'].apply(lambda x: [i.lower() for i in x.split(';;')] )\n",
    "    \n",
    "    pos_lists = []\n",
    "    for userindex, (pos_lis, rewrite_title_lis) in enumerate(zip(df['posnewID'].values.tolist(), df['rewrite_titles'].values.tolist())):\n",
    "        for pos, rewrite_title in zip(pos_lis, rewrite_title_lis):\n",
    "            if rewrite_title.strip() == '':\n",
    "                continue\n",
    "            else:\n",
    "                pos_lists.append([userindex, pos, rewrite_title])\n",
    "    \n",
    "    return df['clicknewsID'].values.tolist(), pos_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 60.8 ms\n"
     ]
    }
   ],
   "source": [
    "%time TestUsers, TestSamples = parse_test_user(testfilename,news_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data2/TestUsers.pkl', 'wb') as f:\n",
    "    pickle.dump(TestUsers, f)\n",
    "with open('../../data2/TestSamples.pkl', 'wb') as f:\n",
    "    pickle.dump(TestSamples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 14111,\n",
       " \"legal battle looms over trump epa's rule change of obama's clean power plan rule\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestSamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ea14d0bd9c7a0f4f2d255c0662c7e1119328c505d1c17d9d8f159415dfcf69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
